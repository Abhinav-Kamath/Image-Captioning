{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBIGanmThXu6"
      },
      "source": [
        "# Importing the required stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugyfN7ttUuLz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import os\n",
        "import string\n",
        "import numpy as np \n",
        "from PIL import Image \n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle \n",
        "import cv2\n",
        "import random\n",
        "import re\n",
        "import torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAwvMhWjqsTz"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGTHcORqquCd",
        "outputId": "0e3d3906-bb1a-4967-ff31-224747b1868a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtaO6PDdooKG"
      },
      "source": [
        "# Image transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfFoFqzfe0U5"
      },
      "outputs": [],
      "source": [
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize(size=256),\n",
        "    transforms.CenterCrop(size=224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd6u7LL1kuCW"
      },
      "source": [
        "# Creating list of images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuwGmKx7mHx8"
      },
      "outputs": [],
      "source": [
        "images_path = \"/content/drive/MyDrive/Flicker8k_Images\"\n",
        "text_path = \"/content/drive/MyDrive/Flickr8k_text/Flickr8k.token.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeRhYhUWuZZX"
      },
      "outputs": [],
      "source": [
        "image_list = []\n",
        "names = []\n",
        "for file in os.listdir(os.path.join(images_path)):\n",
        "    file_path = os.path.join(images_path, file)\n",
        "    image_list.append(img_transform(Image.open(file_path)).unsqueeze(0))\n",
        "    names.append(os.path.basename(file_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0x6LjEDidm9"
      },
      "source": [
        "# Feature extraction using CNN and saving to a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZtt3DZqQBhV"
      },
      "outputs": [],
      "source": [
        "alexnet = models.alexnet(pretrained = True)\n",
        "alexnet.eval()\n",
        "\n",
        "features_dict = {}\n",
        "i = 0\n",
        "for img in image_list:\n",
        "    with torch.no_grad():\n",
        "        feature = alexnet(img).detach().numpy()\n",
        "    name = names[i]\n",
        "    if name not in features_dict.keys():\n",
        "        features_dict[name] = [feature]\n",
        "    else:\n",
        "        features_dict[name].append(feature)\n",
        "print(\"All Training Images Appended!\")\n",
        "\n",
        "with open('features.pickle', 'wb') as handle:\n",
        "    pickle.dump(features_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxuRgT501jjW"
      },
      "source": [
        "# Loading the features file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_M6H27g1l9A"
      },
      "outputs": [],
      "source": [
        "with open('features_2.pickle', 'rb') as handle:\n",
        "    features_dict = pickle.load(handle)\n",
        "print(type(features_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-z9mEXetRuT"
      },
      "source": [
        "# Create a name-caption dictionary and preprocessing captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et-Fq15HsNa-"
      },
      "outputs": [],
      "source": [
        "with open(text_path, \"r\") as f:\n",
        "    captions = f.read().split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeQX-OnH21im"
      },
      "outputs": [],
      "source": [
        "captions_dict = {}\n",
        "i = 0\n",
        "for strn in captions:\n",
        "    contents = strn.split(\"\\t\")\n",
        "    if len(contents) < 2:\n",
        "        continue\n",
        "    filename, cap = contents[0], contents[1]\n",
        "    filename = filename[:-2]\n",
        "    if filename in captions_dict.keys():\n",
        "        captions_dict[filename].append(cap)\n",
        "    else:\n",
        "        captions_dict[filename] = [cap]\n",
        "    if i%2000 == 0:\n",
        "        print(str(i) + \"Captions appended!\")\n",
        "    i += 1\n",
        "print(\"All captions appended!\")\n",
        "with open('features.pickle', 'wb') as handle:\n",
        "    pickle.dump(features_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpgc6E7xAwm0"
      },
      "source": [
        "# Extracting train and test names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsieaWdeFVT6"
      },
      "outputs": [],
      "source": [
        "train_path = \"/content/drive/MyDrive/Flickr8k_text/Flickr_8k.trainImages.txt\"\n",
        "test_path = \"/content/drive/MyDrive/Flickr8k_text/Flickr_8k.testImages.txt\"\n",
        "\n",
        "with open(train_path, \"r\") as f:\n",
        "    train_names = f.read().split(\"\\n\")\n",
        "with open(test_path, \"r\") as f:\n",
        "    test_names = f.read().split(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL8-ubIGHeCo"
      },
      "source": [
        "# Train and test features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRxCH3Pah2av"
      },
      "outputs": [],
      "source": [
        "train_features, train_captions, test_features, test_captions = {}, {}, {}, {}\n",
        "\n",
        "for name in train_names:\n",
        "    if name in features_dict.keys() and name in captions_dict.keys():\n",
        "        train_features[name] = features_dict[name]\n",
        "        train_captions[name] = captions_dict[name]\n",
        "        \n",
        "for name in test_names:\n",
        "    if name in features_dict.keys() and name in captions_dict.keys():\n",
        "        test_features[name] = features_dict[name]\n",
        "        test_captions[name] = captions_dict[name]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUoR1NtIdcTZ"
      },
      "source": [
        "# Creating histogram and vocabulary "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwggBinPdgNh"
      },
      "outputs": [],
      "source": [
        "def preprocess_line(strn):\n",
        "    strn = strn.split()                             \n",
        "    strn = [s.lower() for s in strn]                \n",
        "    strn = [s for s in strn if s.isalpha()]      \n",
        "    strn = \" \".join(strn).translate(\n",
        "        str.maketrans(\"\", \"\", string.punctuation)   # Remove punctuation\n",
        "    )\n",
        "    strn = \"begin \" + strn + \" end\"\n",
        "    \n",
        "    return strn\n",
        "\n",
        "\n",
        "def get_all_captions_tokenize(captions_dict):\n",
        "    captions = []\n",
        "    for cap_lst in captions_dict.values():\n",
        "        captions.extend(cap_lst)\n",
        "    tokenizer = Tokenizer(filters='')           \n",
        "    tokenizer.fit_on_texts(captions)               \n",
        "    vocab_size = len(tokenizer.word_index)+1    \n",
        "    return tokenizer, vocab_size\n",
        "  \n",
        "\n",
        "\n",
        "def pad_tokens(tokens):\n",
        "    return pad_sequences(tokens, padding='post') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOGFcxm1WTYZ"
      },
      "outputs": [],
      "source": [
        "for filename, cap_lst in captions_dict.items():\n",
        "    for i in range(len(cap_lst)):\n",
        "        cap_lst[i] = preprocess_line(cap_lst[i])\n",
        "\n",
        "tokenizer, vocab_size = get_all_captions_tokenize(captions_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2lsFOM1ZZDD"
      },
      "source": [
        "# Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz9RO11HZkwY"
      },
      "outputs": [],
      "source": [
        "def data_generator(img_features, captions_dict, batch_size):\n",
        "    input_img, input_caption, target_cap = [], [], []\n",
        "    count = 0\n",
        "    while True:\n",
        "        for name, caption_list in captions_dict.items():\n",
        "            img_fs = img_features[name]\n",
        "            for cap in caption_list:\n",
        "                caption_seq = tokenizer.texts_to_sequences([cap])[0]\n",
        "                for i in range(1, len(caption_seq)):\n",
        "                    input_seq, trg_seq = caption_seq[:i], caption_seq[i]\n",
        "                    input_img.append(img_fs)\n",
        "                    input_caption.append(input_seq)\n",
        "                    target_cap.append(trg_seq) \n",
        "                    count += 1\n",
        "                    if count == batch_size:\n",
        "                        input_caption = pad_sequences(input_caption, padding='pre')\n",
        "                        yield (\n",
        "                            torch.FloatTensor(np.array(input_img)).squeeze(1).squeeze(1).to('cuda'),\n",
        "                            torch.LongTensor((input_caption)).to('cuda'),\n",
        "                            torch.LongTensor((target_cap)).to('cuda')\n",
        "                        )\n",
        "                        input_img, input_caption, target_cap = [], [], []\n",
        "                        count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK99lUVdc_DT"
      },
      "source": [
        "# Creating Neural Network class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77wIsGradC-D"
      },
      "outputs": [],
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self, glove_weights):\n",
        "        super(Network, self).__init__()\n",
        "        self.fc_img = torch.nn.strnar(1000, 512)               \n",
        "        self.embedding = torch.nn.Embedding(vocab_size, 200)   \n",
        "        self.lstm = torch.nn.LSTM(200, 512, batch_first=True)  \n",
        "        self.fc_wrapper = torch.nn.Linear(1024, 1024)          \n",
        "        self.fc_output = torch.nn.Linear(1024, vocab_size)     \n",
        "        self.embedding.weight = torch.nn.Parameter(glove_weights)\n",
        "\n",
        "    def forward(self, input_img, input_caption):\n",
        "        x1 = self.fc_img(input_img)\n",
        "        x1 = F.relu(x1)\n",
        "        x2 = self.embedding(input_caption)\n",
        "        x2, _ = self.lstm(x2)           \n",
        "        x2 = x2[:, -1, :].squeeze(1)    \n",
        "        x3 = torch.cat((x1, x2), dim=-1)\n",
        "        x3 = self.fc_wrapper(x3)\n",
        "        x3 = F.relu(x3)\n",
        "        x3 = self.fc_output(x3)\n",
        "        out = F.log_softmax(x3, dim=-1)\n",
        "        return out "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4ryKms1Yrmn"
      },
      "source": [
        "#Processing GloVe embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOkPeAmMYq1b"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/glove.6B.200d.txt\", \"r\") as f:\n",
        "    glove = f.read().split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5mDykBwY2hA"
      },
      "outputs": [],
      "source": [
        "glove_dict = {}\n",
        "for strn in glove:\n",
        "    try:\n",
        "        elements = strn.split()\n",
        "        word, vector = elements[0], np.array([float(i) for i in elements[1:]])\n",
        "        glove_dict[word] = vector\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "glove_weights = np.random.uniform(0, 1, (vocab_size, 200))\n",
        "found = 0\n",
        "\n",
        "for word in tokenizer.word_index.keys():\n",
        "    if word in glove_dict.keys():\n",
        "        glove_weights[tokenizer.word_index[word]] = glove_dict[word]\n",
        "        found += 1\n",
        "    else:\n",
        "        continue        \n",
        "print(\"Number of words found in GloVe: {} / {}\".format(found, vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnI6uVdzzJYg"
      },
      "outputs": [],
      "source": [
        "def learning_step(input_img, input_caption, target_cap):\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(input_img, caption_in)\n",
        "    loss = F.nll_loss(preds, target_cap)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lE6iTXQrVsn"
      },
      "outputs": [],
      "source": [
        "epochs = 40\n",
        "steps_per_epoch = len(train_captions)\n",
        "model = Network(glove_weights=torch.FloatTensor(glove_weights).to('cuda'))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "model = model.to(\"cuda\")\n",
        "for epoch in range(epochs):\n",
        "    print(\"Epoch {}\".format(epoch+1))\n",
        "    d_gen = data_generator(train_features, train_captions,32)\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in range(steps_per_epoch):\n",
        "        input_img, input_caption, target_cap = next(d_gen)\n",
        "\n",
        "        input_img.to('cuda')\n",
        "        input_caption.to('cuda')\n",
        "        target_cap.to('cuda')\n",
        "\n",
        "        loss = learning_step(input_img, input_caption, target_cap)\n",
        "        total_loss += loss\n",
        "        if batch % 1000 == 0:\n",
        "            print(\"Epoch {} - Batch {} - Loss {:.4f}\".format(\n",
        "                epoch+1, batch, loss\n",
        "            ))\n",
        "    epoch_loss = total_loss/steps_per_epoch\n",
        "    \n",
        "    print(\"\\nEpoch {} - Average loss {:.4f}\".format(\n",
        "        epoch+1, epoch_loss\n",
        "    ))\n",
        "    if(epoch%10 == 0):\n",
        "        torch.save(model.state_dict(), \"model_{}\".format(epoch+1))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63OYtaqKKfgK"
      },
      "outputs": [],
      "source": [
        "def translate(features):\n",
        "    features = torch.FloatTensor(features)\n",
        "    result = \"begin \"\n",
        "    for t in range(1, max_length):\n",
        "        input_seq = tokenizer.texts_to_sequences([result])\n",
        "        input_seq = pad_sequences(input_seq, maxlen=max_length, padding='pre')\n",
        "        input_seq = torch.LongTensor(input_seq)\n",
        "        preds = model.forward(features, input_seq)\n",
        "        pred_idx = preds.argmax(dim=-1).detach().numpy()[0]\n",
        "        word = tokenizer.index_word.get(pred_idx)\n",
        "        if word is None or word == 'end':\n",
        "            break\n",
        "        result += word + \" \"\n",
        "    return \" \".join(result.split()[1:])\n",
        "\n",
        "def evaluate_model(feature_dict, caption_dict):\n",
        "    refer = []\n",
        "    guess = []\n",
        "    \n",
        "    for name in tqdm(feature_dict.keys()):\n",
        "        prediction = translate(feature_dict[name])\n",
        "        guess.append(prediction.split())\n",
        "        refs = [cap.split() for cap in caption_dict[name]]\n",
        "        refer.append(refs)\n",
        "\n",
        "    bleu_1 = corpus_bleu(refer, guess, weights=(1.0, 0, 0, 0))\n",
        "    bleu_2 = corpus_bleu(refer, guess, weights=(0.5, 0.5, 0, 0))\n",
        "    bleu_3 = corpus_bleu(refer, guess, weights=(0.33, 0.33, 0.33, 0))\n",
        "    bleu_4 = corpus_bleu(refer, guess, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    \n",
        "    print(\"BLEU-1: {:.4f}\".format(bleu_1))\n",
        "    print(\"BLEU-2: {:.4f}\".format(bleu_2))\n",
        "    print(\"BLEU-3: {:.4f}\".format(bleu_3))\n",
        "    print(\"BLEU-4: {:.4f}\".format(bleu_4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRKBjpI7MaxZ"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model_{}\".format(35))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nti6Sj2jFkO"
      },
      "outputs": [],
      "source": [
        "max_length = 20\n",
        "model = Network(glove_weights=torch.FloatTensor(glove_weights))\n",
        "model.load_state_dict(torch.load(\"/content/model\", map_location=torch.device('cuda')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd3lOjAx4BzN"
      },
      "outputs": [],
      "source": [
        "for x in train_features:\n",
        "    val = train_features[x]\n",
        "    train_features[x] = val[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIlD_AE7820r"
      },
      "outputs": [],
      "source": [
        "for x in test_features:\n",
        "    val = test_features[x]\n",
        "    test_features[x] = val[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJbtKaWWn65E"
      },
      "outputs": [],
      "source": [
        "evaluate_model(train_features, train_captions)\n",
        "evaluate_model(test_features, test_captions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9x_uEj43A90"
      },
      "outputs": [],
      "source": [
        "name = list(test_features.keys())[2]\n",
        "image = Image.open(images_path + '/' + name)\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(\"[CAPTION]: {}\".format(translate(test_features[name])))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "VR_mini_project_p1_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}